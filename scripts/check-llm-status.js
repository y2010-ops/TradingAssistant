#!/usr/bin/env node

/**
 * Quick LLM Status Checker
 * Shows which LLMs are currently available and working
 */

import fetch from 'node-fetch';

const colors = {
  reset: '\x1b[0m',
  green: '\x1b[32m',
  red: '\x1b[31m',
  yellow: '\x1b[33m',
  blue: '\x1b[34m',
  cyan: '\x1b[36m',
  bright: '\x1b[1m'
};

const log = {
  info: (msg) => console.log(`${colors.blue}[INFO]${colors.reset} ${msg}`),
  success: (msg) => console.log(`${colors.green}[SUCCESS]${colors.reset} ${msg}`),
  warning: (msg) => console.log(`${colors.yellow}[WARNING]${colors.reset} ${msg}`),
  error: (msg) => console.log(`${colors.red}[ERROR]${colors.reset} ${msg}`),
  header: (msg) => console.log(`${colors.bright}${colors.cyan}${msg}${colors.reset}`)
};

async function checkLLMStatus() {
  log.header('ðŸ¤– AI Trading Assistant - LLM Status Check');
  console.log('='.repeat(60));

  // 1. Check Ollama Status
  log.info('Checking Ollama (Local LLM Server)...');
  try {
    const response = await fetch('http://localhost:11434/api/tags');
    if (response.ok) {
      const data = await response.json();
      
      if (data.models && data.models.length > 0) {
        log.success(`Ollama is running with ${data.models.length} models:`);
        
        for (const model of data.models) {
          const sizeGB = (model.size / (1024 ** 3)).toFixed(1);
          console.log(`  âœ… ${model.name} (${sizeGB}GB)`);
        }
        
        // Test a model
        log.info('Testing model response...');
        const testModel = data.models[0].name;
        const testResponse = await fetch('http://localhost:11434/api/generate', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            model: testModel,
            prompt: 'What is 2+2?',
            stream: false,
            options: { num_predict: 10 }
          })
        });
        
        if (testResponse.ok) {
          const result = await testResponse.json();
          log.success(`Model test successful: "${result.response?.trim()}"`);
        }
        
      } else {
        log.warning('Ollama is running but no models are installed');
        console.log('\nðŸ’¡ To install models, run:');
        console.log('   ollama pull llama3.2:3b    # Recommended');
        console.log('   ollama pull mistral:7b     # High quality');
        console.log('   ollama pull gemma:2b       # Lightweight');
      }
    } else {
      throw new Error(`HTTP ${response.status}`);
    }
  } catch (error) {
    log.error('Ollama is not running or not accessible');
    console.log('\nðŸ’¡ To start Ollama:');
    console.log('   1. Install: curl -fsSL https://ollama.ai/install.sh | sh');
    console.log('   2. Start: ollama serve');
    console.log('   3. Install model: ollama pull llama3.2:3b');
  }

  console.log('\n' + '='.repeat(60));

  // 2. Check Browser-based AI
  log.info('Checking Browser-based AI Models...');
  console.log('  âœ… FinBERT (Financial Sentiment) - Available');
  console.log('  âœ… Sentence Transformers (Embeddings) - Available');
  console.log('  âœ… VADER Sentiment - Available');
  console.log('  âœ… Rule-based Financial Analysis - Available');

  console.log('\n' + '='.repeat(60));

  // 3. Show Current Configuration
  log.info('Current LLM Configuration:');
  
  const config = {
    'Primary LLM': 'Ollama (Local)',
    'Fallback LLM': 'Browser-based Transformers.js',
    'Sentiment Analysis': 'FinBERT + VADER + Custom Lexicon',
    'Embeddings': 'Sentence Transformers (all-MiniLM-L6-v2)',
    'Technical Analysis': 'Custom algorithms + TA-Lib',
    'RAG System': 'FAISS Vector Store + Financial Knowledge Base'
  };

  for (const [key, value] of Object.entries(config)) {
    console.log(`  ðŸ“‹ ${key}: ${value}`);
  }

  console.log('\n' + '='.repeat(60));

  // 4. System Recommendations
  log.info('ðŸ’¡ Recommendations:');
  
  try {
    const ollamaCheck = await fetch('http://localhost:11434/api/tags');
    if (ollamaCheck.ok) {
      const data = await ollamaCheck.json();
      if (data.models && data.models.length > 0) {
        log.success('âœ… Your AI setup is optimal!');
        console.log('   - Local LLM is running');
        console.log('   - Browser AI is available as fallback');
        console.log('   - All analysis engines are ready');
      } else {
        log.warning('âš ï¸  Install at least one model for better performance:');
        console.log('   npm run setup-ollama');
      }
    } else {
      throw new Error('Ollama not running');
    }
  } catch (error) {
    log.warning('âš ï¸  For best performance, set up local LLM:');
    console.log('   1. Run: npm run setup-ollama');
    console.log('   2. Or manually: ollama serve && ollama pull llama3.2:3b');
    console.log('   3. The system will work with browser-based AI only');
  }

  console.log('\n' + '='.repeat(60));
  log.header('ðŸš€ Your AI Trading Assistant is ready!');
}

// Run the check
checkLLMStatus().catch(error => {
  log.error(`Status check failed: ${error.message}`);
  process.exit(1);
});